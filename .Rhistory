libs <- c("readxl",  "torch", "locfit", "purrr", "doParallel", "foreach","dplyr","tibble")
# Kiểm tra và cài đặt nếu thiếu
missing_libs <- setdiff(libs, installed.packages()[,"Package"])
if(length(missing_libs)) install.packages(missing_libs)
# Sau đó nạp thư viện
invisible(lapply(libs, library, character.only = TRUE))
#-----------------------------------------------------------------------------------------
#--- 2. Hàm tiền xử lý dữ liệu ---
prepare_data <- function(filepath){
data <- read_excel(filepath)
names(data) <- tolower(names(data))
data$date <- as.Date(data$date)
data <- data[order(data$date), ]
return(data)
}
#-----------------------------------------------------------------------------------------
#--- 3. Các hàm chức năng
#Hàm DLWR_Fit: tính hồi quy cục bộ động có trọng số cho chuỗi thời gian
dlwr_fit <- function(x, y, n, deg = 1, alpha = 0.3, kern = "tricube", maxit = 30, start_t = 1){
valid_kerns <- c("tricube","gauss","epanech")
if (!kern %in% valid_kerns)
stop(sprintf("kern = '%s' không hợp lệ; chọn một trong: %s",
kern, paste(valid_kerns, collapse = ", ")))
N <- length(x)
y_fitted <- rep(NA, N)
idx_start <- pmax(1, seq(start_t, N) - (n - 1))
idx_end <- seq(start_t, N)
fits <- map2(idx_start, idx_end, function(s, e){
if(e - s + 1 < n) return(NA)
x_sub <- x[s:e]
y_sub <- y[s:e]
if(anyNA(x_sub) || anyNA(y_sub) || any(is.infinite(x_sub)) || any(is.infinite(y_sub))){
return(NA)
}
fit <- tryCatch({
locfit.robust(y_sub ~ lp(x_sub, deg), alpha=alpha, kern=kern, maxit=maxit)
}, error = function(e) NULL)
# Kiểm tra xem fit có thành công không
if(is.null(fit) || !inherits(fit, "locfit")) return(NA)
# Dự báo điểm cuối cùng
pred <- predict(fit, newdata = data.frame(x_sub = x[e]))
if(length(pred) != 1 || any(is.na(pred))) return(NA)
pred
})
y_fitted[idx_end[!is.na(fits)]] <- unlist(fits[!is.na(fits)])
return(y_fitted)
}
#Hàm DLWR_separation: phân rã chuỗi thời gian thành các thành phần nhằm dự báo
dlwr_separation <- function(data, date_col, value_col, params){
data <- data[order(data[[date_col]]), ]
x <- seq_len(nrow(data))
R <- data[[value_col]]
# Vòng 1: start_t = 1
f0 <- dlwr_fit(x, R,
n     = params$n,
deg   = params$deg,
alpha = params$alpha,
kern  = params$kern,
start_t = 1)
d0 <- R - f0
# Vòng 2: start_t = 2*n - 1
f1 <- dlwr_fit(x, d0,
n     = params$n,
deg   = params$deg,
alpha = params$alpha,
kern  = params$kern,
start_t = 2*params$n - 1)
d1 <- d0 - f1
# Vòng 3: start_t = 3*n - 2
f2 <- dlwr_fit(x, d1,
n     = params$n,
deg   = params$deg,
alpha = params$alpha,
kern  = params$kern,
start_t = 3*params$n - 2)
#Phần dư thể hiện nhiễu trắng
d2 <- d1 - f2
cut_idx <- 3 * params$n - 3
result_df <- data.frame(date=data[[date_col]], R, f0, f1, f2, d2)
result_df <- result_df[-seq_len(cut_idx), ]
return(na.omit(result_df))
}
# Chuẩn hóa MinMax về [0,1] để dùng cho deep learning
minmax_scale <- function(x) {
min_val <- min(x, na.rm = TRUE)
max_val <- max(x, na.rm = TRUE)
scaled <- (x - min_val) / (max_val - min_val)
list(scaled = scaled, min = min_val, max = max_val)
}
# Khôi phục lại giá trị gốc sau khi dự báo bằng deep learning
minmax_inverse <- function(scaled_x, min_val, max_val) {
scaled_x * (max_val - min_val) + min_val
}
# Chia series thành train, validation, test
split_series <- function(series, test_days = 65, val_ratio = 0.1) {
N <- length(series)
if (test_days >= N) {
stop("Số ngày test lớn hơn hoặc bằng độ dài chuỗi.")
}
# Tập train+val là phần còn lại
train_val_end <- N - test_days
idx_val <- floor(train_val_end * val_ratio)
if (idx_val >= train_val_end) {
stop("Tập validation quá lớn so với tập train.")
}
list(
train = series[1:(train_val_end - idx_val)],
val   = series[(train_val_end - idx_val + 1):train_val_end],
test  = series[(train_val_end + 1):N]
)
}
#tạo bộ dữ liệu phù hợp cho deep learning
create_dataset <- function(series, lookback, horizon = 1, device) {
# Ép về float và đưa lên device
ts <- torch_tensor(series, dtype = torch_float(), device = device)
n   <- ts$size()[1] - lookback - horizon + 1L
if (n <= 0) stop("Chuỗi quá ngắn cho lookback + horizon")
X <- vector("list", n)
Y <- vector("list", n)
for (i in seq_len(n)) {
seq_x <- ts[i:(i + lookback - 1)]             # [lookback]
seq_y <- ts[(i + lookback):(i + lookback + horizon - 1)]  # [horizon]
X[[i]] <- seq_x$unsqueeze(2)                  # [lookback, 1]
Y[[i]] <- seq_y$unsqueeze(2)                  # [horizon, 1]
}
x_t <- torch_stack(X)                           # [batch, lookback, 1]
y_t <- torch_stack(Y)                           # [batch, horizon, 1]
# nếu bạn muốn y là [batch, horizon]:
y_t <- y_t$squeeze(3)
list(x = x_t, y = y_t)
}
#Thiết kế module LSTM phù hợp
LSTMModel <- nn_module(
"LSTMModel",
initialize = function(horizon = 1) {
self$horizon <- horizon
self$lstm1 <- nn_lstm(input_size = 1, hidden_size = 64, batch_first = TRUE)
self$lstm2 <- nn_lstm(input_size = 64, hidden_size = 128, batch_first = TRUE)
self$lstm3 <- nn_lstm(input_size = 128, hidden_size = 128, batch_first = TRUE)
self$fc1 <- nn_linear(128, 128)
self$dropout <- nn_dropout(0.2)
self$fc2 <- nn_linear(128, horizon)  # output multi-step
},
forward = function(x) {
x <- self$lstm1(x)[[1]]
x <- self$lstm2(x)[[1]]
x <- self$lstm3(x)[[1]]
x <- x[, dim(x)[2], ]  # lấy timestep cuối
x <- self$fc1(x)
x <- nnf_relu(x)       # ReLU sau fc1
x <- self$dropout(x)
x <- self$fc2(x)
x <- nnf_relu(x)       # ReLU sau fc2
x
}
)
#Tính chỉ tiêu đo lường hiệu quả mô hình: R^2
r_squared <- function(y_true, y_pred, ref_mean = NULL) {
if (is.null(ref_mean)) ref_mean <- mean(y_true, na.rm = TRUE)  # mặc định cũ
ss_res <- sum((y_true - y_pred)^2, na.rm = TRUE)
ss_tot <- sum((y_true - ref_mean)^2, na.rm = TRUE)
1 - ss_res / ss_tot
}
# Định nghĩa các thành phần cho học sâu: LSTM
train_lstm_component <- function(series,
lookback =10,
horizon          = 1,
epochs           = 20,
batch_size       = 16,
lr               = 1e-3,
device,
test_days = 65,
patience         = 10,
min_epochs       = 10,
val_loss_thres   = Inf,
restore_best     = TRUE,
hidden_size      = 128,
num_layers       = 2,
cell             = c("lstm","gru"),
verbose          = TRUE) {
stopifnot(length(series) > lookback + horizon)
## 1. scale & split
splits <- if (test_days > 0)
split_series(series, test_days = test_days, val_ratio = 0.1)
else
split_series(series, test_days = 0,           val_ratio = 0.1)
# Lấy riêng train và val
train <- splits$train
val   <- splits$val
test   <- splits$test
## 2) Scale lần lượt train và val
sc       <- minmax_scale(train)        # <-- chỉ train
train_sc <- sc$scaled
val_sc   <- (val - sc$min) / (sc$max - sc$min)
#stopifnot(min(length(train), length(val), length(test)) > lookback)
## 2. dataset & loader
ds_train <- create_dataset(train_sc, lookback, horizon, device)
ds_val   <- create_dataset(val_sc,   lookback, horizon, device)
hist_series <- c(train_sc, val_sc)
if (length(hist_series) < lookback + horizon)
stop("hist_series quá ngắn – giảm lookback hoặc bổ sung dữ liệu")
ds_test  <- create_dataset(
hist_series[(length(hist_series) - lookback - horizon + 1) :
length(hist_series)],        # ít nhất lookback+horizon điểm
lookback, horizon, device)
train_dl <- dataloader(tensor_dataset(ds_train$x, ds_train$y),
batch_size, shuffle = TRUE)
val_dl   <- dataloader(tensor_dataset(ds_val$x, ds_val$y),
batch_size)
## 3. model / optim
#model <- LSTMModel(horizon = horizon)$to(device = device, dtype = torch_float())
#model <- LSTMModel(horizon = horizon)$to(device, dtype = torch_float())
model <- LSTMModel(horizon = horizon)
model <- model$to(device = device)
opt       <- optim_adam(model$parameters, lr = lr)
loss_fn   <- nn_mse_loss()
best_val  <- Inf; best_state <- NULL; wait <- 0
best_state  <- NULL
wait        <- 0
for (ep in seq_len(epochs)) {
### train
model$train()
coro::loop(for (b in train_dl) {
b <- lapply(b, function(t) t$to(device = device))
opt$zero_grad()
loss <- loss_fn(model(b[[1]]), b[[2]])
loss$backward()
opt$step()
})
### validate
model$eval()
vlosses <- c()
with_no_grad({
coro::loop(for (b in val_dl) {
b <- lapply(b, function(t) t$to(device = device))
vlosses <- c(vlosses, as.numeric(loss_fn(model(b[[1]]), b[[2]])))
})
})
v <- mean(vlosses)
if (verbose) cat(sprintf("Epoch %d  val_loss = %.5g\n", ep, v))
if (v < best_val - 1e-8) {
best_val <- v; best_state <- model$state_dict(); wait <- 0
} else wait <- wait + 1
if (ep >= min_epochs && (best_val < val_loss_thres || wait >= patience))
break
}
if (restore_best) model$load_state_dict(best_state)
## 4. predict test
model$eval()
with_no_grad({
preds_sc <- model(ds_test$x)$detach()  # [n_test, horizon]
})
if (horizon == 1) {
# Trường hợp chỉ dự báo 1 bước: giữ dạng vector và inverse-scale trực tiếp
preds <- minmax_inverse(as.numeric(preds_sc), sc$min, sc$max)
} else {
# Nhiều bước: kết quả là ma trận, inverse-scale theo cột
preds <- apply(as.matrix(preds_sc), 2,
minmax_inverse, sc$min, sc$max, simplify = TRUE)
# Bảo đảm vẫn là ma trận & gán tên cột
preds <- matrix(preds,
ncol     = horizon,
dimnames = list(NULL, paste0("t+", 1:horizon)))
}
list(preds   = preds,          # matrix: [n_test, horizon]
val_loss = best_val,
scaler   = sc,
model_state = best_state)
}
predict_dlwr_lstm <- function(series,
dlwr_par  = list(n = 15, deg = 1, alpha = 0.2,
kern = "tricube"),
lstm_par  = list(lookback = 15, epochs = 50,
batch_size = 16, lr = 1e-3),
seed      = 26,
test_days = 65,
val_ratio = 0.2,
verbose   = TRUE) {
## --- 0. chuẩn bị ---------------------------------------------------------
stopifnot(is.numeric(series), length(series) > dlwr_par$n * 3)
if (verbose) cat("Series length:", length(series), "\n")
set.seed(seed); torch_manual_seed(seed)
device <- torch_device(if (cuda_is_available()) "cuda" else "cpu")
## --- 1. phân rã DLWR -----------------------------------------------------
df     <- data.frame(date = seq_along(series), close = series)
res    <- dlwr_separation(df, "date", "close", dlwr_par)
# 2) Tách chỉ để giữ y_true cuối cùng
splits_full <- split_series(res$R, test_days = test_days, val_ratio = val_ratio)
y_true <- splits_full$test
# 3) Recursive forecast helper:
#    - Huấn luyện trên train+val (toàn phần trước test)
#    - Lặp test_days lần: mỗi lần predict 1 bước, append vào input
forecast_comp <- function(comp_vec) {
# split train+val
trval <- head(comp_vec, length(comp_vec) - test_days)
#c     <- minmax_scale(trval)
#sc_ts  <- sc$scaled
# train LSTM trên toàn trval
out <- train_lstm_component(
series     = trval,
lookback   = lstm_par$lookback,
epochs     = lstm_par$epochs,
batch_size = lstm_par$batch_size,
lr         = lstm_par$lr,
device     = device,
test_days   = 0,
verbose    = FALSE
)
model_state <- out$model_state
scaler      <- out$scaler
# rebuild model and load best weights
model <- LSTMModel(horizon = 1)
model <- model$to(device = device)
model$load_state_dict(model_state)
model$eval()
# bắt đầu recursive
hist_scaled <- torch_tensor(
(trval - scaler$min) / (scaler$max - scaler$min),
dtype = torch_float())$to(device = device)
preds <- numeric(test_days)
for (t in seq_len(test_days)) {
# lấy lookback cuối cùng
input_seq <- hist_scaled[(length(hist_scaled) - lstm_par$lookback + 1):length(hist_scaled)]
input_seq <- input_seq$unsqueeze(1)$unsqueeze(3)  # [1, lookback, 1]
with_no_grad({
out_t <- model(input_seq)$squeeze() |> as.numeric()
})
# inverse scale
pred_t <- scaler$min + out_t * (scaler$max - scaler$min)
preds[t] <- pred_t
# append vào hist_scaled for next step
new_scaled <- (pred_t - scaler$min) / (scaler$max - scaler$min)
hist_scaled <- torch_cat(list(hist_scaled, torch_tensor(new_scaled)$to(device=device)), dim = 1)
}
#preds <- minmax_inverse(preds, scaler$min, scaler$max)
preds
}
# 4) Forecast 4 thành phần
pf0 <- forecast_comp(res$f0)
pf1 <- forecast_comp(res$f1)
pf2 <- forecast_comp(res$f2)
pd2 <- forecast_comp(res$d2)
print(paste("pf0: ", pf0))
print(paste("pf1: ", pf1))
print(paste("pf2: ", pf2))
print(paste("pd2: ", pd2))
comp_list  <- list(pf0 = pf0, pf1 = pf1, pf2 = pf2, pd2 = pd2)
lens       <- sapply(comp_list, length)
n_na       <- sapply(comp_list, function(x) sum(is.na(x)))
cat("Chiều dài: ", paste(names(lens), lens,  sep = "=", collapse = ", "), "\n")
cat("NA       : ", paste(names(n_na),  n_na,  sep = "=", collapse = ", "), "\n")
stopifnot(all(lens == test_days) && all(n_na == 0))
# 5) Ghép lại và tính metrics
stopifnot(length(pf0) == length(pf1),
length(pf1) == length(pf2),
length(pf2) == length(pd2),
length(pf0) == test_days)
y_pred <- pf0 + pf1 + pf2 + pd2
print(paste0("y_pred =",y_pred,"; y_true = ", y_true))
MAE  <- mean(abs(y_true - y_pred))
RMSE <- sqrt(mean((y_true - y_pred)^2))
MAPE <- mean(abs((y_true - y_pred) / y_true)) * 100
mean_train <- mean(head(res$R, length(res$R) - test_days), na.rm = TRUE)
R2   <- r_squared(y_true, y_pred, mean_train)
metrics_vec <- c(MAE = MAE,
RMSE = RMSE,
MAPE = MAPE,
R2   = R2)
list(seed    = seed,
metrics = c(MAE = MAE, RMSE = RMSE, MAPE = MAPE, R2 = R2),
y_true  = y_true,
y_pred  = y_pred)
}
#--- 6. Auto-tune Parallel ---
auto_tune_serial_gpu <- function(dlwr_params, lstm_params, series,
n_seeds = 50) {
# 1) Tạo các grid
dlwr_grid <- expand.grid(
n     = dlwr_params$n,
deg   = dlwr_params$deg,
kern  = dlwr_params$kern,
alpha = dlwr_params$alpha,
stringsAsFactors = FALSE
)
lstm_grid <- expand.grid(
lookback   = lstm_params$lookback_grid,
epochs     = lstm_params$epochs_grid,
batch_size = lstm_params$batch_size_grid,
lr         = lstm_params$lr_grid,
stringsAsFactors = FALSE
)
combos <- merge(dlwr_grid, lstm_grid)
# 2) Chọn device
device <- torch_device(if (cuda_is_available()) "cuda" else "cpu")
cat("Using device:", device$type, "\n")
results <- list()
idx <- 1L
# 3) Duyệt qua từng tổ hợp
for (i in seq_len(nrow(combos))) {
hp <- combos[i, ]
dlwr_p <- as.list(hp[c("n","deg","alpha","kern")])
lstm_p <- as.list(hp[c("lookback","epochs","batch_size","lr")])
# 3.1. Phân rã bằng DLWR
sep <- dlwr_separation(
data      = data.frame(date = seq_along(series), close = series),
date_col  = "date",
value_col = "close",
params    = dlwr_p
)
if (nrow(sep) <= 65) {
message("→ Sep còn", nrow(sep), " (<65) – bỏ combo này")
next                       # bỏ qua tổ hợp HP hiện tại
}
cat(sprintf(
"Combo %d/%d  DLWR(n=%d,deg=%d,kern=%s,α=%.2f)  LSTM(lb=%d,ep=%d,bs=%d,lr=%g)\n",
i, nrow(combos),
hp$n, hp$deg, hp$kern, hp$alpha,
hp$lookback, hp$epochs, hp$batch_size, hp$lr
))
# 3.2. Duyệt seed
set.seed(123)
seed_vec <- sample.int(1e6, n_seeds)
for (sd in seed_vec) {#seq_len(lstm_params$n_jobs * 0 + 50)) {  # 50 seeds
out <- predict_dlwr_lstm(
series    = series,
dlwr_par  = dlwr_p,
lstm_par  = lstm_p,
seed      = sd,
test_days = lstm_params$test_days %||% 65,
verbose   = FALSE
)
R2 <- out$metrics["R2"]
print(R2)
if (!is.na(R2) && R2 >= lstm_params$target_R2 %||% 0.99) {
results[[idx]] <- tibble::tibble(
seed       = sd,
n          = hp$n,
deg        = hp$deg,
kern       = hp$kern,
lookback   = hp$lookback,
epochs     = hp$epochs,
batch_size = hp$batch_size,
lr         = hp$lr,
MAE        = out$metrics["MAE"],
RMSE       = out$metrics["RMSE"],
MAPE       = out$metrics["MAPE"],
R2         = R2
)
idx <- idx + 1L
}
}
}
# 4) Collate & sort
final <- dplyr::bind_rows(results)
final %>%
arrange(desc(R2), MAPE, RMSE) %>%
dplyr::mutate(rank = dplyr::row_number())
}
#--- 7. Chạy pipeline hoàn chỉnh ---
device <- torch_device(if (cuda_is_available()) "cuda" else "cpu")
dlwr_params <- list(n=10, deg=c(1,2), alpha=c(0.1,0.2), kern = c("tricube", "gauss"))
lstm_params <- list(
lookback_grid   = c(5, 10, 15, 20),
epochs_grid     = c(20),
batch_size_grid = c(16),
lr_grid         = c(0.0005, 0.001),
val_loss_threshold   = 0.0001,
patience             = 5,
restore_best_weights = TRUE,
device               = device,
n_jobs               = 4,
verbose              = TRUE
)
raw_data <- prepare_data("data_vnindex1.xlsx")   # hoặc file bạn đang dùng
results <- auto_tune_serial_gpu(dlwr_params, lstm_params,
raw_data$vnindex_close,
n_seeds = 1)
source("~/.active-rstudio-document")
results <- predict_dlwr_lstm(
raw_data$vnindex_close,
dlwr_par = list(n = 5, deg = 1, alpha = 0.2, kern = "tricube"), # n nhỏ hơn
lstm_par = list(lookback = 10, epochs = 40, batch_size = 16, lr = 5e-4),
seed = 123, verbose = FALSE)
results <- predict_dlwr_lstm(
raw_data$vnindex_close,
dlwr_par = list(n = 10, deg = 1, alpha = 0.2, kern = "tricube"),
lstm_par = list(lookback = 10, epochs = 20,
batch_size = 16, lr = 5e-4),
seed = 123, verbose = FALSE)
print(results$metrics)
#  ↳ Bạn nên thấy MAPE ≈ 1–1.5 %,  R2 ≈ 0.97–0.99
device <- torch_device(if (cuda_is_available()) "cuda" else "cpu")
dlwr_params <- list(n=10, deg=c(1,2), alpha=0.2, kern = c("tricube", "gauss"))
lstm_params <- list(
lookback_grid   = c(10, 15, 20),
epochs_grid     = c(20),
batch_size_grid = c(16),
lr_grid         = c(0.0005, 0.001),
val_loss_threshold   = 0.0001,
patience             = 5,
restore_best_weights = TRUE,
device               = device,
n_jobs               = 4,
verbose              = TRUE
)
results <- auto_tune_serial_gpu(dlwr_params, lstm_params,
raw_data$vnindex_close,
n_seeds = 1)
print(results$metrics)
source("~/.active-rstudio-document")
q
quit()
