###########################
# 1. C√†i ƒë·∫∑t v√† n·∫°p c√°c g√≥i c·∫ßn thi·∫øt
###########################
packages <- c("readxl", "xts", "zoo", "tseries", "rugarch", "forecast", "FinTS","Rlibeemd","torch")
installed <- rownames(installed.packages())
for (pkg in packages) {
  if (!(pkg %in% installed)) install.packages(pkg)
  library(pkg, character.only = TRUE)
}
tf <- tensorflow::tf
#use_python("C:/Users/quanp/AppData/Local/Programs/Python/Python312/python.exe", required = TRUE)

# N·∫øu b·∫°n ƒë√£ c√†i Python v√† tensorflow t·ª´ tr∆∞·ªõc, c√≥ th·ªÉ khai b√°o r√µ:
# library(reticulate)
# use_python("C:/Users/quanp/AppData/Local/Programs/Python/Python312/python.exe", required = TRUE)
#install_keras(tensorflow = "2.16.1")
###########################
# 2. ƒê·ªçc v√† x·ª≠ l√Ω d·ªØ li·ªáu
###########################
data <- read_excel("data_vnindex1.xlsx")
names(data) <- tolower(names(data))
data$date <- as.Date(data$date)
data <- data[order(data$date), ]
data_xts <- xts(data[, setdiff(names(data), "date")], order.by = data$date)

###########################
# 3. Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra
###########################
training_data <- data_xts["/2024-03-31"]
test_data     <- data_xts["2024-04-01/"]

# Ki·ªÉm ƒë·ªãnh t√≠nh d·ª´ng chu·ªói log-return
log_return_train <- diff(log(training_data$vnindex_close))
log_return_train <- na.omit(log_return_train)

cat("\n--- Ki·ªÉm ƒë·ªãnh t√≠nh d·ª´ng ---\n")
cat("ADF test:\n")
print(adf.test(log_return_train))
cat("\nKPSS test:\n")
print(kpss.test(log_return_train))

###########################
# 4. ∆Ø·ªõc l∆∞·ª£ng m√¥ h√¨nh ARIMA ƒë·ªÉ l·∫•y th√¥ng s·ªë
###########################
fit_arima <- auto.arima(log_return_train)
print(summary(fit_arima))
arima_order <- arimaorder(fit_arima)

###########################
# 5. ƒê·ªãnh nghƒ©a c√°c m√¥ h√¨nh GARCH bi·∫øn th·ªÉ
###########################
spec_arima_garch <- ugarchspec(
  variance.model = list(model = "sGARCH", garchOrder = c(1,1)),
  mean.model = list(armaOrder = c(arima_order[1], arima_order[3]), include.mean = TRUE),
  distribution.model = "std"
)

spec_arima_egarch <- ugarchspec(
  variance.model = list(model = "eGARCH", garchOrder = c(1,1)),
  mean.model = list(armaOrder = c(arima_order[1], arima_order[3]), include.mean = TRUE),
  distribution.model = "std"
)

spec_arima_gjr <- ugarchspec(
  variance.model = list(model = "gjrGARCH", garchOrder = c(1,1)),
  mean.model = list(armaOrder = c(arima_order[1], arima_order[3]), include.mean = TRUE),
  distribution.model = "std"
)

spec_garch <- ugarchspec(
  variance.model = list(model = "sGARCH", garchOrder = c(1,1)),
  mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),
  distribution.model = "std"
)

###########################
# 6. D·ª± b√°o rolling t·ª´ng b∆∞·ªõc cho c√°c m√¥ h√¨nh GARCH
###########################
forecast_horizon <- nrow(test_data)
last_train_value <- as.numeric(tail(training_data$vnindex_close, 1))
actual <- as.numeric(test_data$vnindex_close)
forecast_dates <- index(test_data)[1:forecast_horizon]

models <- list(
  ARIMA = fit_arima,
  GARCH = spec_garch,
  ARIMA_GARCH = spec_arima_garch,
  ARIMA_EGARCH = spec_arima_egarch,
  ARIMA_GJR_GARCH = spec_arima_gjr
)

results <- list()

# ARIMA d·ª± b√°o ri√™ng
forecast_arima <- forecast(models$ARIMA, h = forecast_horizon)
results[["ARIMA"]] <- exp(cumsum(as.numeric(forecast_arima$mean))) * last_train_value

# GARCH-type models d·ª± b√°o rolling
for (name in names(models)[-1]) {
  cat("Running model:", name, "\n")
  roll <- ugarchroll(
    models[[name]],
    data = log_return_train,
    n.ahead = 1,
    forecast.length = forecast_horizon,
    refit.every = 5,
    refit.window = "moving",
    solver = "hybrid"
  )
  mu <- as.numeric(roll@forecast$density[,"Mu"])
  mu_drift_adj <- mu - mean(mu)
  level <- exp(cumsum(mu_drift_adj)) * last_train_value
  results[[name]] <- level
}

###########################
# 7. X√¢y v√† hu·∫•n luy·ªán m√¥ h√¨nh LSTM (n·∫øu kh·∫£ d·ª•ng)
###########################
device <- if (cuda_is_available()) torch_device("cuda") else torch_device("cpu")
cat("Thi·∫øt b·ªã ƒëang d√πng:", device$type, "\n")

# ------------------------------
# 1. D·ªØ li·ªáu & CEEMDAN ph√¢n r√£
# ------------------------------
raw_series <- as.numeric(training_data$vnindex_close)
forecast_horizon <- as.integer(nrow(test_data))
imfs <- ceemdan(raw_series, ensemble_size = 250L, noise_strength = 0.2)
num_imfs <- min(nrow(imfs), 6)  # Gi·ªõi h·∫°n s·ªë IMFs ƒë·ªÉ tƒÉng hi·ªáu nƒÉng

# ------------------------------
# 2. H√†m chu·∫©n h√≥a MinMax
# ------------------------------
scale_minmax <- function(x) {
  min_x <- min(x, na.rm = TRUE)
  max_x <- max(x, na.rm = TRUE)
  range <- max_x - min_x
  if (range == 0) range <- 1
  scaled <- (x - min_x) / range
  list(scaled = scaled, min = min_x, max = max_x)
}

# ------------------------------
# 3. Chu·∫©n b·ªã d·ªØ li·ªáu LSTM
# ------------------------------
create_lstm_data <- function(series, lag = 10L) {
  X <- list()
  y <- c()
  for (i in 1:(length(series) - lag)) {
    X[[i]] <- series[i:(i + lag - 1)]
    y[i] <- series[i + lag]
  }
  X_array <- array(unlist(X), dim = c(length(X), lag, 1))
  y_array <- y
  X_tensor <- torch_tensor(X_array, dtype = torch_float())$to(device = device)
  y_tensor <- torch_tensor(y_array, dtype = torch_float())$to(device = device)
  list(X = X_tensor, y = y_tensor)
}

# ------------------------------
# 4. M√¥ h√¨nh LSTM ƒë∆°n gi·∫£n
# ------------------------------
lstm_model <- nn_module(
  initialize = function() {
    self$lstm <- nn_lstm(input_size = 1, hidden_size = 32, batch_first = TRUE)
    self$fc <- nn_linear(32, 1)
  },
  forward = function(x) {
    out <- self$lstm(x)[[1]]
    last <- out[ , dim(out)[2], ]
    self$fc(last)
  }
)

# ------------------------------
# 5. Hu·∫•n luy·ªán v√† d·ª± b√°o 1 IMF
# ------------------------------
train_predict_lstm <- function(
    scaled_series,
    lag = 10L,
    horizon = 30L,
    epochs = 100L,
    batch_size = 16L,
    early_stopping_patience = 10,
    min_delta = 1e-5,
    verbose = TRUE
) {
  # 1. T·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán
  data <- create_lstm_data(scaled_series, lag)
  X <- data$X
  y <- data$y
  
  # 2. Kh·ªüi t·∫°o m√¥ h√¨nh v√† t·ªëi ∆∞u
  model <- lstm_model()
  model$to(device = device)
  optimizer <- optim_adam(model$parameters, lr = 0.001)
  loss_fn <- nn_mse_loss()
  
  best_loss <- Inf
  patience_counter <- 0
  
  # 3. Hu·∫•n luy·ªán m√¥ h√¨nh
  for (epoch in 1:epochs) {
    model$train()
    optimizer$zero_grad()
    
    output <- model(X)
    output <- output$view(c(-1))  # chuy·ªÉn v·ªÅ vector c√πng shape v·ªõi y
    
    loss <- loss_fn(output, y)
    loss$backward()
    
    nn_utils_clip_grad_norm_(model$parameters, max_norm = 1.0)
    optimizer$step()
    
    current_loss <- loss$item()
    if (verbose && epoch %% 10 == 0) {
      cat(sprintf("Epoch %d: loss = %.6f\n", epoch, current_loss))
    }
    
    if (best_loss - current_loss > min_delta) {
      best_loss <- current_loss
      patience_counter <- 0
    } else {
      patience_counter <- patience_counter + 1
    }
    
    if (patience_counter >= early_stopping_patience) {
      if (verbose) cat("Early stopping t·∫°i epoch", epoch, "\n")
      break
    }
  }
  
  # 4. D·ª± b√°o chu·ªói t∆∞∆°ng lai
  model$eval()
  preds <- numeric(horizon)
  last_seq <- tail(scaled_series, lag)
  
  for (i in 1:horizon) {
    last_seq <- as.numeric(last_seq)
    
    if (length(last_seq) != lag) {
      stop("L·ªói: ƒê·ªô d√†i chu·ªói kh√¥ng ƒë√∫ng t·∫°i b∆∞·ªõc ", i)
    }
    
    if (any(!is.finite(last_seq))) {
      stop("L·ªói: Chu·ªói ch·ª©a gi√° tr·ªã kh√¥ng h·ª£p l·ªá t·∫°i b∆∞·ªõc ", i)
    }
    
    input_array <- array(last_seq, dim = c(1, lag, 1))
    mode(input_array) <- "numeric"
    
    if (typeof(input_array) != "double") {
      stop("L·ªói: input_array kh√¥ng ph·∫£i ki·ªÉu double t·∫°i b∆∞·ªõc ", i)
    }
    
    input_tensor <- tryCatch({
      torch_tensor(input_array, dtype = torch_float())$to(device = device)
    }, error = function(e) {
      stop("L·ªói khi t·∫°o tensor t·∫°i b∆∞·ªõc ", i, ": ", conditionMessage(e))
    })
    
    pred <- as.numeric(model(input_tensor)$item())
    
    if (!is.finite(pred)) {
      stop("L·ªói: D·ª± b√°o kh√¥ng h·ª£p l·ªá t·∫°i b∆∞·ªõc ", i)
    }
    
    preds[i] <- pred
    last_seq <- c(last_seq[-1], pred)
  }
  
  # 5. Gi·∫£i ph√≥ng b·ªô nh·ªõ GPU n·∫øu c·∫ßn
  rm(model)
  gc()
  torch::cuda_empty_cache()
  
  return(preds)
}



# ------------------------------
# 6. Hu·∫•n luy·ªán t·ª´ng IMF v√† t·ªïng h·ª£p
# ------------------------------
preds_components <- list()
for (i in 1:num_imfs) {
  cat("üì¶ D·ª± b√°o IMF", i, "\n")
  comp <- imfs[i, ]
  scaled <- scale_minmax(comp)
  pred_scaled <- train_predict_lstm(
    scaled_series = scaled$scaled,
    lag = 10L,
    horizon = forecast_horizon,
    epochs = 50
  )
  
  # Ph·ª•c h·ªìi thang ƒëo ban ƒë·∫ßu
  pred_unscaled <- pred_scaled * (scaled$max - scaled$min) + scaled$min
  preds_components[[paste0("IMF", i)]] <- pred_unscaled
}

# ------------------------------
# 7. T·ªïng h·ª£p d·ª± b√°o cu·ªëi c√πng
# ------------------------------
final_forecast <- Reduce("+", preds_components)
results[["CEEMDAN_LSTM"]] <- final_forecast



library(keras)
library(tensorflow)

#########################
# 1. H√†m ph√¢n t√°ch DLWR
#########################
DLWR_decompose <- function(R, spans = c(0.3, 0.3, 0.3), degree = 2) {
  n <- length(R)
  t <- seq_len(n)
  fit_f0 <- loess(R ~ t, span = spans[1], degree = degree)
  f0 <- predict(fit_f0, newdata = data.frame(t = t))
  d0 <- R - f0
  fit_f1 <- loess(d0 ~ t, span = spans[2], degree = degree)
  f1 <- predict(fit_f1, newdata = data.frame(t = t))
  d1 <- d0 - f1
  fit_f2 <- loess(d1 ~ t, span = spans[3], degree = degree)
  f2 <- predict(fit_f2, newdata = data.frame(t = t))
  d2 <- d1 - f2
  return(list(f0 = f0, f1 = f1, f2 = f2, d2 = d2))
}

#########################
# 2. Ti·ªán √≠ch h·ªó tr·ª£
#########################
scale_minmax <- function(x) {
  min_x <- min(x, na.rm = TRUE)
  max_x <- max(x, na.rm = TRUE)
  scaled <- (x - min_x) / (max_x - min_x)
  list(scaled = scaled, min = min_x, max = max_x)
}

create_lstm_data <- function(series, lag = 10L) {
  n <- length(series)
  num_samples <- n - lag
  X <- array(NA, dim = c(num_samples, lag, 1))
  y <- array(NA, dim = c(num_samples))
  for (i in 1:num_samples) {
    X[i, , 1] <- series[i:(i + lag - 1)]
    y[i] <- series[i + lag]
  }
  list(X = X, y = y)
}

build_lstm_model <- function(input_shape = c(10L, 1L)) {
  model <- keras_model_sequential()
  model$add(layer_lstm(units = 64, input_shape = input_shape, return_sequences = TRUE))
  model$add(layer_lstm(units = 128, return_sequences = TRUE))
  model$add(layer_lstm(units = 128))
  model$add(layer_dense(units = 128, activation = "relu"))
  model$add(layer_dropout(rate = 0.2))
  model$add(layer_dense(units = 1, activation = "relu"))
  return(model)
}



train_predict_lstm <- function(scaled_series, lag = 10L, horizon = 30L, epochs = 100, batch_size = 32) {
  data <- create_lstm_data(scaled_series, lag)
  X <- data$X
  y <- data$y
  
  model <- build_lstm_model(input_shape = c(lag, 1))
  model$compile(
    loss = "mean_squared_error",
    optimizer = keras$optimizers$Adam()
  )
  
  early_stop <- callback_early_stopping(
    monitor = "loss",
    patience = 10,
    restore_best_weights = TRUE
  )
  
  model$fit(
    x = X,
    y = y,
    epochs = 100L,
    batch_size = 32L,
    callbacks = list(early_stop)
  )
  
  preds <- numeric(horizon)
  last_seq <- tail(scaled_series, lag)
  for (i in 1:horizon) {
    input_seq <- array(last_seq, dim = c(1L, lag, 1L))
    input_seq <- tf$cast(input_seq, dtype = tf$float32)
    pred <- as.numeric(model$predict(input_seq))
    preds[i] <- pred
    last_seq <- c(last_seq[-1], pred)
  }
  
  return(preds)
}

#########################
# 3. B·∫Øt ƒë·∫ßu tri·ªÉn khai m√¥ h√¨nh DLWR-LSTM
#########################

# D·ªØ li·ªáu ƒë·∫ßu v√†o
raw_series <- as.numeric(training_data$vnindex_close)
decomp <- DLWR_decompose(raw_series, spans = c(0.3, 0.3, 0.3), degree = 2)

# S·ªë b∆∞·ªõc d·ª± b√°o
forecast_horizon <- as.integer(nrow(test_data))

# Kh·ªüi t·∫°o danh s√°ch k·∫øt qu·∫£
preds_components <- list()

# Hu·∫•n luy·ªán v√† d·ª± b√°o t·ª´ng th√†nh ph·∫ßn DLWR b·∫±ng LSTM ri√™ng
for (name in c("f0", "f1", "f2", "d2")) {
  comp <- decomp[[name]]
  scaled <- scale_minmax(comp)
  pred_scaled <- train_predict_lstm(
    scaled_series = scaled$scaled,
    lag = 10L,
    horizon = forecast_horizon,
    epochs = 100,
    batch_size = 32
  )
  preds_components[[name]] <- pred_scaled * (scaled$max - scaled$min) + scaled$min
}

# T·ªïng h·ª£p d·ª± b√°o cu·ªëi c√πng
final_forecast <- preds_components$f0 + preds_components$f1 + preds_components$f2 + preds_components$d2

#########################
# 4. V·∫Ω k·∫øt qu·∫£ so v·ªõi th·ª±c t·∫ø
#########################

actual <- as.numeric(test_data$vnindex_close)
forecast_dates <- index(test_data)

plot(forecast_dates, actual, type = "l", col = "blue", lwd = 2,
     main = "D·ª± b√°o DLWR-LSTM v·ªõi 4 th√†nh ph·∫ßn",
     xlab = "Ng√†y", ylab = "VN-Index",
     ylim = range(c(actual, final_forecast)))
lines(forecast_dates, final_forecast, col = "red", lwd = 2)
legend("topleft", legend = c("Th·ª±c t·∫ø", "DLWR-LSTM D·ª± b√°o"),
       col = c("blue", "red"), lty = 1, lwd = 2)

# L∆∞u k·∫øt qu·∫£
results[["LSTM_LWR"]] <- final_forecast



library(torch)
library(Rlibeemd)

# 1. D·ªØ li·ªáu v√† thi·∫øt l·∫≠p
raw_series <- as.numeric(training_data$vnindex_close)
forecast_horizon <- as.integer(nrow(test_data))

# 2. CEEMDAN ph√¢n r√£ chu·ªói
imfs <- ceemdan(raw_series, ensemble_size = 250L, noise_strength = 0.2)
num_imfs <- nrow(imfs)
max_imfs <- min(num_imfs, 6)  # ch·ªâ d√πng 6 IMF ƒë·∫ßu
preds_components <- list()

# 3. Scale MinMax
scale_minmax <- function(x) {
  min_x <- min(x, na.rm = TRUE)
  max_x <- max(x, na.rm = TRUE)
  scaled <- (x - min_x) / (max_x - min_x)
  list(scaled = scaled, min = min_x, max = max_x)
}

# 4. T·∫°o d·ªØ li·ªáu cho LSTM
create_lstm_data <- function(series, lag = 10L) {
  n <- length(series)
  X <- list()
  y <- c()
  for (i in 1:(n - lag)) {
    X[[i]] <- series[i:(i + lag - 1)]
    y[i] <- series[i + lag]
  }
  X_tensor <- torch_tensor(array(unlist(X), dim = c(length(X), lag, 1)), dtype = torch_float())
  y_tensor <- torch_tensor(y, dtype = torch_float())
  list(X = X_tensor, y = y_tensor)
}

# 5. M√¥ h√¨nh LSTM
lstm_model <- nn_module(
  initialize = function() {
    self$lstm1 <- nn_lstm(input_size = 1, hidden_size = 64, batch_first = TRUE)
    self$lstm2 <- nn_lstm(input_size = 64, hidden_size = 128, batch_first = TRUE)
    self$fc1 <- nn_linear(128, 64)
    self$drop <- nn_dropout(p = 0.2)
    self$fc2 <- nn_linear(64, 1)
  },
  forward = function(x) {
    x <- self$lstm1(x)[[1]]
    x <- self$lstm2(x)[[1]]
    x <- x[ , dim(x)[2], ]
    x <- nnf_relu(self$fc1(x))
    x <- self$drop(x)
    self$fc2(x)
  }
)

# 6. Hu·∫•n luy·ªán LSTM tr√™n t·ª´ng IMF
train_predict_lstm_torch <- function(scaled_series, lag = 10L, horizon = 30L,
                                     epochs = 30, batch_size = 16, device = torch_device("cpu")) {
  data <- create_lstm_data(scaled_series, lag)
  X <- data$X$to(device = device)
  y <- data$y$to(device = device)
  
  model <- lstm_model()
  model$to(device = device)
  
  optimizer <- optim_adam(model$parameters, lr = 0.01)
  loss_fn <- nn_mse_loss()
  
  for (epoch in 1:epochs) {
    model$train()
    optimizer$zero_grad()
    pred <- model(X)$squeeze()
    loss <- loss_fn(pred, y)
    loss$backward()
    optimizer$step()
    if (epoch %% 10 == 0) cat(sprintf("Epoch %d - Loss: %.6f\n", epoch, loss$item()))
  }
  
  # D·ª± b√°o
  preds <- c()
  last_seq <- tail(scaled_series, lag)
  model$eval()
  for (i in 1:horizon) {
    input <- torch_tensor(array(last_seq, dim = c(1, lag, 1)), dtype = torch_float())$to(device = device)
    pred <- model(input)
    pred_val <- as.numeric(pred$item())
    preds[i] <- pred_val
    last_seq <- c(last_seq[-1], pred_val)
  }
  
  rm(model); gc(); torch::cuda_empty_cache()
  return(preds)
}

# 7. D·ª± b√°o l·∫ßn l∆∞·ª£t 6 IMF ƒë·∫ßu
device <- if (cuda_is_available()) torch_device("cuda") else torch_device("cpu")
for (i in 1:max_imfs) {
  cat(sprintf("üß† Hu·∫•n luy·ªán IMF %d / %d\n", i, max_imfs))
  comp <- imfs[i, ]
  scaled <- scale_minmax(comp)
  pred_scaled <- train_predict_lstm_torch(
    scaled_series = scaled$scaled,
    lag = 10L,
    horizon = forecast_horizon,
    epochs = 30,
    batch_size = 16,
    device = device
  )
  preds_components[[i]] <- pred_scaled * (scaled$max - scaled$min) + scaled$min
}

# 8. T·ªïng h·ª£p d·ª± b√°o
final_forecast <- Reduce(`+`, preds_components)

# 9. V·∫Ω k·∫øt qu·∫£
actual <- as.numeric(test_data$vnindex_close)
forecast_dates <- index(test_data)

plot(forecast_dates, actual, type = "l", col = "blue", lwd = 2,
     main = "CEEMDAN-LSTM torch (6 IMF)",
     xlab = "Ng√†y", ylab = "VN-Index",
     ylim = range(c(actual, final_forecast)))
lines(forecast_dates, final_forecast, col = "red", lwd = 2)
legend("topleft", legend = c("Th·ª±c t·∫ø", "CEEMDAN-LSTM (6 IMF)"),
       col = c("blue", "red"), lwd = 2)

# 10. L∆∞u k·∫øt qu·∫£
results[["CEEMDAN_LSTM_6IMF"]] <- final_forecast






###########################
# 8. V·∫Ω bi·ªÉu ƒë·ªì v·ªõi t·∫•t c·∫£ m√¥ h√¨nh
###########################
plot(forecast_dates, actual, type="l", col="blue", lwd=2,
     main="D·ª± b√°o VN-Index v·ªõi ARIMA, GARCH v√† LSTM",
     xlab="Ng√†y", ylab="VN-Index",
     ylim=range(c(actual, unlist(results))))

lines(forecast_dates, results[["ARIMA"]], col="gray", lwd=2, lty=1)
lines(forecast_dates, results[["GARCH"]], col="black", lwd=2, lty=5)
lines(forecast_dates, results[["ARIMA_GARCH"]], col="darkorange", lwd=2, lty=2)
lines(forecast_dates, results[["ARIMA_EGARCH"]], col="purple", lwd=2, lty=3)
lines(forecast_dates, results[["ARIMA_GJR_GARCH"]], col="green", lwd=2, lty=4)

if ("LSTM_DLWR" %in% names(results)) {
  lines(forecast_dates, results[["LSTM_DLWR"]], col="magenta", lwd=2, lty=7)
}

legend("topleft",
       legend=c("Th·ª±c t·∫ø", "ARIMA", "GARCH", "ARIMA-GARCH", "ARIMA-EGARCH", "ARIMA-GJR-GARCH", "LSTM", "LSTM-DLWR"),
       col=c("blue", "gray", "black", "darkorange", "purple", "green", "red", "magenta"),
       lty=c(1,1,5,2,3,4,6,7), lwd=2)
